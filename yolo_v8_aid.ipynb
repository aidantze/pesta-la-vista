{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzOjj4t0Nd9ROMxng2mNAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidantze/pesta-la-vista/blob/yolo_v8/yolo_v8_aid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo v8"
      ],
      "metadata": {
        "id": "1VOqp0_A0OaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "pJYzlJpuMfHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO constants\n",
        "RESOLUTION = 512\n",
        "EPOCHS = 1                        # set to 10 for testing, leave as 30 default\n",
        "\n",
        "# Noise and filter constants\n",
        "APPLY_CORRUPTION = True\n",
        "CORRUPT_TYPE = 'gaussian_noise'   # one of: 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'\n",
        "CORRUPT_STRENGTH = 0.05           # e.g., 0.05 = 5% noise or 5x5 kernel blur"
      ],
      "metadata": {
        "id": "1OsTCRT8Mei2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "3Yt9hHP10TS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-w-ocdOwJhpM",
        "outputId": "c19b4074-5c73-47ca-c007-b899c910481d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Using Colab cache for faster access to the 'crop-pests-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics kagglehub pyyaml\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "import pathlib\n",
        "import yaml\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Check for T4 availability on Colab (DON'T CHANGE)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected. Go to Runtime -> Change runtime type ->  GPU\")\n",
        "\n",
        "# Kaggle Download via CLI API (see their website - DON'T CHANGE)\n",
        "path = kagglehub.dataset_download(\"rupankarmajumdar/crop-pests-dataset\")\n",
        "\n",
        "# Saved the images to a local path to increase efficiency\n",
        "local_path = pathlib.Path(\"/content/datasets/crop-pests\")\n",
        "local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, local_path, dirs_exist_ok=True)\n",
        "\n",
        "# YAML CONGIF (DON'T CHANGE)\n",
        "data_yaml_path = local_path / \"data.yaml\"\n",
        "data_cfg = {\n",
        "    \"path\": str(local_path),\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\":   \"valid/images\",\n",
        "    \"test\":  \"test/images\",\n",
        "    \"names\": [\n",
        "        \"ant\", \"bee\", \"beetle\", \"caterpillar\", \"earthworm\", \"earwig\",\n",
        "        \"grasshopper\", \"moth\", \"slug\", \"snail\", \"wasp\", \"weevil\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(data_cfg, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise and Filter/Blur Analysis"
      ],
      "metadata": {
        "id": "MCCHT7HH0-E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Noise and filter/blur analysis\n",
        "def apply_corruption_to_folder(source_dir, destination_dir, corruption_type, strength=0.01):\n",
        "    \"\"\"\n",
        "    Copies images from source to destination and applies a specified corruption.\n",
        "\n",
        "    Args:\n",
        "        source_dir (pathlib.Path): Directory containing original images.\n",
        "        destination_dir (pathlib.Path): Target directory for corrupted images.\n",
        "        corruption_type (str): 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'.\n",
        "        strength (float/int): Magnitude of the corruption.\n",
        "    \"\"\"\n",
        "    if destination_dir.exists():\n",
        "        shutil.rmtree(destination_dir)\n",
        "    shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "    if corruption_type == 'gaussian_noise':\n",
        "        # Mean=0, standard deviation=strength * 255\n",
        "        sigma = int(strength * 255)\n",
        "        print(f\"\\nApplying {corruption_type} (Sigma: {sigma}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            noise = np.random.normal(0, sigma, img.shape).astype('uint8')\n",
        "            corrupted_img = cv2.add(img, noise)\n",
        "\n",
        "            # Save the corrupted image, overwriting the copy\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'salt_pepper_noise':\n",
        "        ratio = strength # Ratio of pixels to corrupt\n",
        "        print(f\"\\nApplying {corruption_type} (Ratio: {ratio}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            corrupted_img = img.copy()\n",
        "            total_pixels = img.size\n",
        "            num_salt_pepper = int(ratio * total_pixels / img.shape[2]) # Total pixels / num channels\n",
        "\n",
        "            # Salt noise (white)\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 255\n",
        "\n",
        "            # Pepper noise (black)\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 0\n",
        "\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'gaussian_blur':\n",
        "        # Kernel size = strength (must be odd), sigma=0 (auto)\n",
        "        ksize = int(strength * 100) if strength % 2 != 0 else int(strength * 100) + 1\n",
        "        print(f\"\\nApplying {corruption_type} (ksize: {ksize}) to images in {destination_dir.name}...\")\n",
        "\n",
        "        for img_file in destination_dir.glob('*.jpg'): # Adjust extension if needed\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            corrupted_img = cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
        "\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    print(\"Corruption application complete.\")\n",
        "\n",
        "\n",
        "if (APPLY_CORRUPTION):\n",
        "  # Create a new validation directory for the corrupted test set\n",
        "  original_val_path = local_path / \"valid\" / \"images\"\n",
        "  corrupt_val_path = local_path / \"valid_noisy\" / \"images\"\n",
        "\n",
        "  # Apply corruption to the copied test set\n",
        "  # We use 'valid' here since YOLOv8 validation defaults to the 'val' split name.\n",
        "  apply_corruption_to_folder(\n",
        "      original_val_path,\n",
        "      corrupt_val_path,\n",
        "      CORRUPT_TYPE,\n",
        "      CORRUPT_STRENGTH\n",
        "  )\n",
        "\n",
        "  # Update the YAML to point to the corrupted validation set for the experiment\n",
        "  data_cfg_corrupted = data_cfg.copy()\n",
        "  data_cfg_corrupted['val'] = \"valid_noisy/images\"\n",
        "  data_yaml_path_corrupted = local_path / \"data_corrupted.yaml\"\n",
        "\n",
        "  with open(data_yaml_path_corrupted, \"w\") as f:\n",
        "      yaml.safe_dump(data_cfg_corrupted, f)\n",
        "  print(\"Wrote corrupted config:\", data_yaml_path_corrupted)"
      ],
      "metadata": {
        "id": "kA1KUeVg0Kia",
        "outputId": "7ebd210d-d90a-4b8f-a005-31ab7e45c380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying gaussian_noise (Sigma: 12) to images in images...\n",
            "Corruption application complete.\n",
            "Wrote corrupted config: /content/datasets/crop-pests/data_corrupted.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model and prediction"
      ],
      "metadata": {
        "id": "uro6Ql8Z1Ai3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model (as per YOLOv8n website)\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# See changes made to increase for Colab\n",
        "start = time.time()\n",
        "train_res = model.train(\n",
        "    data=str(data_yaml_path),\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=RESOLUTION,\n",
        "    batch=-1,            # auto batch size based on GPU memory\n",
        "    device=0,            # use GPU (CUDA:0)\n",
        "    workers=4,           # increase data loading threads\n",
        "    cache=True,          # cache dataset in RAM/Disk for speed\n",
        "    amp=True,            # mixed precision\n",
        "    freeze=10,           # aka for transfer learning (recommended by AI)\n",
        "    mosaic=0.5,          # light augmentations\n",
        "    mixup=0.0,\n",
        "    copy_paste=0.0,\n",
        "    project=\"pests_fast\",\n",
        "    name=\"yolov8n_colab\",\n",
        "    plots=False\n",
        ")\n",
        "end = time.time()\n",
        "train_time = end - start\n",
        "\n",
        "# Validate the Model (as per YOLOv8n website)\n",
        "start = time.time()\n",
        "val_results = model.val(\n",
        "    data=str(data_yaml_path),\n",
        "    split=\"test\",\n",
        "    batch=16,\n",
        "    device=0,\n",
        "    workers=2\n",
        ")\n",
        "end = time.time()\n",
        "val_time = end - start\n",
        "print(val_results)\n",
        "\n",
        "# Test the Model (as per YOLOv8n website)\n",
        "sample_dir = local_path / \"test\" / \"images\"\n",
        "start = time.time()\n",
        "test_results = model.predict(\n",
        "    source=str(sample_dir),\n",
        "    batch=16,\n",
        "    imgsz=RESOLUTION,\n",
        "    device=0,\n",
        "    save=True,\n",
        "    project=\"runs/detect\",\n",
        "    name=\"pest_predictions\"\n",
        ")\n",
        "end = time.time()\n",
        "test_time = end - start"
      ],
      "metadata": {
        "id": "6BqpsDnu0KT1",
        "outputId": "588ef008-5e1b-456e-a25e-5b74ca7a8892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.227 ðŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/datasets/crop-pests/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=yolov8n_colab11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=pests_fast, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/pests_fast/yolov8n_colab11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=12\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753652  ultralytics.nn.modules.head.Detect           [12, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,188 parameters, 3,013,172 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 9.2Â±4.1 MB/s, size: 51.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/crop-pests/train/labels.cache... 11502 images, 3 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 11502/11502 5.3Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=512 at 60.0% CUDA memory utilization.\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla T4) 14.74G total, 0.76G reserved, 0.10G allocated, 13.87G free\n",
            "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
            "     3013188       5.252         0.474         28.25         114.4        (1, 3, 512, 512)                    list\n",
            "     3013188        10.5         0.505         20.93         12.33        (2, 3, 512, 512)                    list\n",
            "     3013188       21.01         0.512         18.11         12.48        (4, 3, 512, 512)                    list\n",
            "     3013188       42.01         0.671         19.59         19.11        (8, 3, 512, 512)                    list\n",
            "     3013188       84.03         1.026         38.06         28.47       (16, 3, 512, 512)                    list\n",
            "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 203 for CUDA:0 8.84G/14.74G (60%) âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 11.2Â±3.8 MB/s, size: 44.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/crop-pests/train/labels.cache... 11502 images, 3 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 11502/11502 16.6Mit/s 0.0s\n",
            "WARNING âš ï¸ \u001b[34m\u001b[1mtrain: \u001b[0m12.6GB RAM required to cache images with 50% safety margin but only 3.0/12.7GB available, not caching images\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 11.4Â±7.2 MB/s, size: 35.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/crop-pests/valid/labels.cache... 1095 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1095/1095 1.4Mit/s 0.0s\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.8GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1095/1095 78.1it/s 14.0s\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0015859375), 63 bias(decay=0.0)\n",
            "Image sizes 512 train, 512 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/pests_fast/yolov8n_colab11\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/1      6.96G      1.649       4.11      1.978        440        512: 0% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0/57  2.9s"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output Metrics"
      ],
      "metadata": {
        "id": "yyEZLxUzLD6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Average Precision (mAP)\n",
        "mAP50 = val_results.results_dict['metrics/mAP50(B)']\n",
        "mAP50_95 = val_results.results_dict['metrics/mAP50-95(B)']\n",
        "\n",
        "# Precision, Recall, and F1-score\n",
        "precision = val_results.results_dict['metrics/precision(B)']\n",
        "recall = val_results.results_dict['metrics/recall(B)']\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Note: YOLO does not directly output a single 'Accuracy' metric in the classification sense,\n",
        "# nor a single 'Area Under the Curve (AUC)' value; mAP is the primary AUC equivalent.\n",
        "\n",
        "print(f\"Results for YOLO with {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "print(f\"Mean Average Precision (mAP@0.50): {mAP50:.4f}\")\n",
        "print(f\"Mean Average Precision (mAP@0.50-0.95): {mAP50_95:.4f}\")\n",
        "print(f\"Precision (Box): {precision:.4f}\")\n",
        "print(f\"Recall (Box): {recall:.4f}\")\n",
        "print(f\"F1-Score (Derived): {f1_score:.4f}\\n\")\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Converts total seconds into minutes and format.\"\"\"\n",
        "    mins, secs = divmod(seconds, 60)\n",
        "    return f\"{int(mins):0d}m {secs:.2f}s\"\n",
        "\n",
        "print(f\"Training Time (Total): {format_time(train_time)}\")\n",
        "print(f\"Testing/Validation Time (Total): {format_time(test_time)}\")"
      ],
      "metadata": {
        "id": "vT5Z2W_LLAX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}