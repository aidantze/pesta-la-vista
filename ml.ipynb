{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0qBDfp9duyMcVsJnvaulq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidantze/pesta-la-vista/blob/ml_models/ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Models\n",
        "9517 Group Assignment"
      ],
      "metadata": {
        "id": "opIMolq4bNlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "YAGFwJFMaEMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN constants\n",
        "RESOLUTION = 64                  # default 512\n",
        "K_NEIGHBORS = 5                   # default 5\n",
        "\n",
        "# Noise and filter constants\n",
        "APPLY_CORRUPTION = False\n",
        "CORRUPT_TYPE = 'gaussian_noise'   # one of: 'gaussian_noise', 'salt_pepper_noise', 'gaussian_blur'\n",
        "CORRUPT_STRENGTH = 0.05           # e.g., 0.05 = 5% noise or 5x5 kernel blur"
      ],
      "metadata": {
        "id": "HlD-ljheZq9_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "M022Oz4xaHL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub pyyaml\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "import pathlib\n",
        "import yaml\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import torch\n",
        "\n",
        "# Check for T4 availability on Colab (DON'T CHANGE)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected. Go to Runtime -> Change runtime type ->  GPU\")\n",
        "\n",
        "# Kaggle Download via CLI API (see their website - DON'T CHANGE)\n",
        "path = kagglehub.dataset_download(\"rupankarmajumdar/crop-pests-dataset\")\n",
        "\n",
        "# Saved the images to a local path to increase efficiency\n",
        "local_path = pathlib.Path(\"/content/datasets/crop-pests\")\n",
        "local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, local_path, dirs_exist_ok=True)\n",
        "\n",
        "# YAML CONGIF (DON'T CHANGE)\n",
        "data_yaml_path = local_path / \"data.yaml\"\n",
        "data_cfg = {\n",
        "    \"path\": str(local_path),\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\":   \"valid/images\",\n",
        "    \"test\":  \"test/images\",\n",
        "    \"names\": [\n",
        "        \"ant\", \"bee\", \"beetle\", \"caterpillar\", \"earthworm\", \"earwig\",\n",
        "        \"grasshopper\", \"moth\", \"slug\", \"snail\", \"wasp\", \"weevil\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(data_cfg, f)\n",
        ""
      ],
      "metadata": {
        "id": "zk1KaTwOaILg",
        "outputId": "d4345c33-1e17-4da6-e5f4-0e193ac7e993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "No GPU detected. Go to Runtime -> Change runtime type ->  GPU\n",
            "Using Colab cache for faster access to the 'crop-pests-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading Function for KNN\n",
        "CLASS_NAMES = data_cfg[\"names\"]\n",
        "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "# def load_split_data(base_path, split_name, class_map, img_size):\n",
        "#     \"\"\"Loads and preprocesses images and labels for a specific data split.\"\"\"\n",
        "#     X_list = []\n",
        "#     y_list = []\n",
        "\n",
        "#     # We must traverse the actual class folders within the split (e.g., train/ant, train/bee)\n",
        "#     split_dir = base_path / split_name\n",
        "\n",
        "#     for class_name, label_id in class_map.items():\n",
        "#         image_folder = split_dir / \"images\" / class_name\n",
        "#         print(image_folder)\n",
        "\n",
        "#         if image_folder.exists():\n",
        "#             for img_file in image_folder.glob(\"*.jpg\"):\n",
        "#                 img = cv2.imread(str(img_file), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#                 if img is not None:\n",
        "#                     # Resize, flatten, and append\n",
        "#                     img = cv2.resize(img, (img_size, img_size))\n",
        "#                     img_normalized = img.astype('float32') / 255.0\n",
        "\n",
        "#                     X_list.append(img_normalized.flatten())\n",
        "#                     y_list.append(label_id)\n",
        "\n",
        "#     X = np.array(X_list, dtype='float32')\n",
        "#     y = np.array(y_list)\n",
        "\n",
        "#     # Normalize pixel values (0-255 -> 0.0-1.0)\n",
        "#     # X /= 255.0\n",
        "\n",
        "#     # print(f\"Loaded {len(X)} samples for {split_name}.\")\n",
        "#     # return X, y\n",
        "\n",
        "#     if X.ndim == 1 and len(X) > 0:\n",
        "#         # This handles the rare case where np.array(X_list) fails to stack\n",
        "#         # (e.g., if X_list contains varying types). We reshape explicitly.\n",
        "#         # This requires all elements in X_list to have the same length.\n",
        "#         try:\n",
        "#             target_features = img_size * img_size * 3\n",
        "#             X = X.reshape(-1, target_features)\n",
        "#             print(\"Successfully reshaped 1D array to 2D.\")\n",
        "#         except ValueError as e:\n",
        "#             print(f\"Error during final reshape check: {e}\")\n",
        "#             # The issue might be inconsistent feature counts.\n",
        "#             return np.array([]), np.array([])\n",
        "\n",
        "#     print(f\"Loaded {len(X)} samples for {split_name}. Final shape: {X.shape}\")\n",
        "#     return X, y\n",
        "\n",
        "def load_split_data(base_path, split_name, class_map, img_size):\n",
        "    \"\"\"\n",
        "    Loads images and extracts the majority class label from the YOLO-style\n",
        "    label files for KNN classification.\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    split_dir = base_path / split_name\n",
        "    image_dir = split_dir / \"images\"\n",
        "    label_dir = split_dir / \"labels\"\n",
        "\n",
        "    print(f\"\\nProcessing images in: {image_dir}\")\n",
        "\n",
        "    if not image_dir.exists() or not label_dir.exists():\n",
        "        print(f\"Error: Could not find 'images' or 'labels' in {split_dir}\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    for img_file in image_dir.glob(\"*.jpg\"):\n",
        "\n",
        "        # 1. Determine the corresponding label file name (.txt)\n",
        "        label_file = label_dir / (img_file.stem + \".txt\")\n",
        "\n",
        "        # 2. Check and process label file\n",
        "        if not label_file.exists():\n",
        "            # print(f\"Warning: Missing label file for {img_file.name}\") # Too verbose\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read all lines from the label file\n",
        "            with open(label_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            if not lines:\n",
        "                continue # Skip if label file is empty\n",
        "\n",
        "            # Extract all class IDs (the first number in each row)\n",
        "            class_ids = [int(line.strip().split()[0]) for line in lines]\n",
        "\n",
        "            # Find the majority class ID (the most frequent insect)\n",
        "            # bincount is efficient for finding frequencies of non-negative integers\n",
        "            class_counts = np.bincount(class_ids)\n",
        "            majority_class_id = np.argmax(class_counts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading label {label_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # 3. Load and Preprocess Image\n",
        "        # Load as color image (3 channels) for consistent feature count\n",
        "        img = cv2.imread(str(img_file), cv2.IMREAD_COLOR)\n",
        "\n",
        "        if img is not None:\n",
        "            # Resize, flatten, and normalize\n",
        "            img = cv2.resize(img, (img_size, img_size))\n",
        "            img_normalized = img.astype('float32') / 255.0\n",
        "\n",
        "            X_list.append(img_normalized.flatten())\n",
        "            y_list.append(majority_class_id) # Use the derived majority class\n",
        "\n",
        "    # 4. Final array creation (handles the 2D shape requirement)\n",
        "    X = np.array(X_list, dtype='float32')\n",
        "    y = np.array(y_list)\n",
        "\n",
        "    print(f\"Loaded {len(X)} samples for {split_name}. Final shape: {X.shape}\")\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "l8zo0YMDkvr5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise and Filter Analysis"
      ],
      "metadata": {
        "id": "fjEM2WtaaQT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_corruption_to_folder(source_dir, destination_dir, corruption_type, strength=0.01):\n",
        "    \"\"\"\n",
        "    Copies images from source to destination and applies a specified corruption.\n",
        "    (Function body is omitted here for brevity, assuming the user's provided code is used)\n",
        "    \"\"\"\n",
        "    if destination_dir.exists():\n",
        "        shutil.rmtree(destination_dir)\n",
        "    shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "    if corruption_type == 'gaussian_noise':\n",
        "        sigma = int(strength * 255)\n",
        "        print(f\"\\nApplying {corruption_type} (Sigma: {sigma}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            noise = np.random.normal(0, sigma, img.shape).astype('uint8')\n",
        "            corrupted_img = cv2.add(img, noise)\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'salt_pepper_noise':\n",
        "        ratio = strength\n",
        "        print(f\"\\nApplying {corruption_type} (Ratio: {ratio}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            corrupted_img = img.copy()\n",
        "            total_pixels = img.size\n",
        "            num_salt_pepper = int(ratio * total_pixels / img.shape[2])\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 255\n",
        "            coords = [np.random.randint(0, i - 1, num_salt_pepper) for i in img.shape]\n",
        "            corrupted_img[coords[0], coords[1], coords[2]] = 0\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    elif corruption_type == 'gaussian_blur':\n",
        "        ksize = int(strength * 100) if int(strength * 100) % 2 != 0 else int(strength * 100) + 1\n",
        "        print(f\"\\nApplying {corruption_type} (ksize: {ksize}) to images in {destination_dir.name}...\")\n",
        "        for img_file in destination_dir.glob('*.jpg'):\n",
        "            img = cv2.imread(str(img_file))\n",
        "            if img is None: continue\n",
        "            corrupted_img = cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
        "            cv2.imwrite(str(img_file), corrupted_img)\n",
        "\n",
        "    print(\"Corruption application complete.\")\n",
        "\n",
        "\n",
        "VAL_DIR_NAME = \"valid\"\n",
        "if APPLY_CORRUPTION:\n",
        "    original_val_path = local_path / VAL_DIR_NAME\n",
        "    corrupt_val_path_name = f\"{VAL_DIR_NAME}_corrupted\"\n",
        "    corrupt_val_path = local_path / corrupt_val_path_name\n",
        "\n",
        "    # Create the corrupted validation set by copying and modifying the 'valid' split\n",
        "    apply_corruption_to_folder(\n",
        "        original_val_path,\n",
        "        corrupt_val_path,\n",
        "        CORRUPT_TYPE,\n",
        "        CORRUPT_STRENGTH\n",
        "    )\n",
        "    VAL_DIR_NAME = corrupt_val_path_name # Switch the validation directory name"
      ],
      "metadata": {
        "id": "-QCrLSKbaSqU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data Splits"
      ],
      "metadata": {
        "id": "LiJYeVN0aW1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Load Data Splits ---\n",
        "X_train, y_train = load_split_data(local_path, \"train\", CLASS_MAP, RESOLUTION)\n",
        "\n",
        "# Load validation data (clean or corrupted)\n",
        "X_val, y_val = load_split_data(local_path, VAL_DIR_NAME, CLASS_MAP, RESOLUTION)\n",
        "\n",
        "# Load test data\n",
        "X_test, y_test = load_split_data(local_path, \"test\", CLASS_MAP, RESOLUTION)"
      ],
      "metadata": {
        "id": "9H1ZJQY9aYKs",
        "outputId": "46da735e-9e08-4134-f3bd-8eaf72f7ba61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing images in: /content/datasets/crop-pests/train/images\n",
            "Loaded 11499 samples for train. Final shape: (11499, 12288)\n",
            "\n",
            "Processing images in: /content/datasets/crop-pests/valid/images\n",
            "Loaded 1095 samples for valid. Final shape: (1095, 12288)\n",
            "\n",
            "Processing images in: /content/datasets/crop-pests/test/images\n",
            "Loaded 546 samples for test. Final shape: (546, 12288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Classifier â€” Train model, prediction and evaluation"
      ],
      "metadata": {
        "id": "C6OKAJK2acIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=K_NEIGHBORS)\n",
        "\n",
        "start = time.time()\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "train_time = end - start\n",
        "\n",
        "start_kernel = time.time()\n",
        "knn_val_predictions = knn_classifier.predict(X_val)\n",
        "end = time.time()\n",
        "test_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "knn_val_probas = knn_classifier.predict_proba(X_val)\n",
        "end = time.time()\n",
        "val_time = end - start\n",
        "\n",
        "# knn_val_predictions = knn_classifier.predict(X_val)\n",
        "# knn_val_probas = knn_classifier.predict_proba(X_val)\n",
        "\n",
        "# def evaluate_knn(X, y, dataset_name, classifier):\n",
        "#     \"\"\"Predicts and prints metrics for a given dataset split.\"\"\"\n",
        "#     start_time = time.time()\n",
        "#     predictions = classifier.predict(X)\n",
        "#     runtime = time.time() - start_time\n",
        "\n",
        "#     print(f\"\\n=== KNN ({dataset_name}) ===\")\n",
        "#     print(f\"Runtime: {runtime:.2f} seconds\")\n",
        "#     print(\"Accuracy:\", metrics.accuracy_score(y, predictions))\n",
        "#     print(\"\\nClassification Report:\")\n",
        "#     print(metrics.classification_report(y, predictions, target_names=CLASS_NAMES))\n",
        "\n",
        "#     return runtime\n",
        "\n",
        "# # Validate the Model\n",
        "# val_time = evaluate_knn(X_val, y_val, \"Validation Data\", knn_classifier)\n",
        "\n",
        "# # Predict/Test the Model\n",
        "# test_time = evaluate_knn(X_test, y_test, \"Test Data\", knn_classifier)\n"
      ],
      "metadata": {
        "id": "_PPB92IEailE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "# We use the 'weighted' average suitable for multi-class, potentially imbalanced data.\n",
        "accuracy = metrics.accuracy_score(y_val, knn_val_predictions)\n",
        "precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    y_val, knn_val_predictions, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "# AUC is calculated in a One-vs-Rest fashion for multi-class problems.\n",
        "# The true labels (y_val) must be binarized first.\n",
        "lb = LabelBinarizer()\n",
        "y_val_binarized = lb.fit_transform(y_val)\n",
        "\n",
        "# We use the 'weighted' average to account for class support\n",
        "auc_score = metrics.roc_auc_score(\n",
        "    y_val_binarized,\n",
        "    knn_val_probas,\n",
        "    average='weighted',\n",
        "    multi_class='ovr'\n",
        ")\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Converts total seconds into minutes and seconds format.\"\"\"\n",
        "    # Ensure all time variables (train_time, val_time, test_time) are available\n",
        "    mins, secs = divmod(seconds, 60)\n",
        "    return f\"{int(mins):0d}m {secs:.2f}s\"\n",
        "\n",
        "\n",
        "# --- 4. Output Refactored Metrics ---\n",
        "print(f\"Results for KNN (K={K_NEIGHBORS}) with image size {RESOLUTION} and {f\"{CORRUPT_TYPE}, strength set to {CORRUPT_STRENGTH}\" if APPLY_CORRUPTION else \"no filters\"}:\\n\")\n",
        "\n",
        "# NOTE: mAP is not available for KNN (Classification)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_score:.4f}\")\n",
        "print(f\"Area Under the Curve (AUC): {auc_score:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time (Total): {format_time(train_time)}\")\n",
        "print(f\"Validation Time (Prediction): {format_time(val_time)}\")"
      ],
      "metadata": {
        "id": "XU32-Ylor7Lr",
        "outputId": "7cc0ec81-3dbf-4b8d-8749-1de0ac635381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for KNN (K=5) with image size 64 and no filters:\n",
            "\n",
            "Accuracy: 0.1653\n",
            "Precision (Weighted): 0.1874\n",
            "Recall (Weighted): 0.1653\n",
            "F1-Score (Weighted): 0.1505\n",
            "Area Under the Curve (AUC): 0.5918\n",
            "\n",
            "Training Time (Total): 0m 0.12s\n",
            "Validation Time (Prediction): 0m 12.28s\n"
          ]
        }
      ]
    }
  ]
}