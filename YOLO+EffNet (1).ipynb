{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fvZr6HrUde_",
        "outputId": "23c37872-568b-436c-9934-a495b90ee86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Using Colab cache for faster access to the 'crop-pests-dataset' dataset.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 112.1MB/s 0.1s\n",
            "Ultralytics 8.3.228 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/datasets/crop-pests/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo_effnet, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/yolo_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/yolo_runs/yolo_effnet, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 24.5MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=12\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753652  ultralytics.nn.modules.head.Detect           [12, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,188 parameters, 3,013,172 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 113.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1602.8Â±669.1 MB/s, size: 47.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/crop-pests/train/labels... 11502 images, 3 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 11502/11502 2.2Kit/s 5.2s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/crop-pests/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 671.2Â±165.6 MB/s, size: 34.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/crop-pests/valid/labels... 1095 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1095/1095 1.1Kit/s 1.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/crop-pests/valid/labels.cache\n",
            "Plotting labels to /content/yolo_runs/yolo_effnet/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/yolo_runs/yolo_effnet\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/15      1.18G      1.507      3.265      1.847         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.1it/s 3:56\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 6.9it/s 10.0s\n",
            "                   all       1095       1341      0.476      0.395      0.399      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/15      1.38G      1.498      2.609      1.816         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.3it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 7.9it/s 8.8s\n",
            "                   all       1095       1341      0.616      0.441      0.487      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/15       1.4G      1.494      2.389      1.805         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.4it/s 3:45\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 7.9it/s 8.8s\n",
            "                   all       1095       1341      0.582      0.463      0.508      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/15      1.42G       1.48      2.242      1.791         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.3it/s 3:47\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 7.9it/s 8.7s\n",
            "                   all       1095       1341      0.629      0.572       0.58      0.305\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/15      1.44G       1.45      2.075       1.77         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.3it/s 3:47\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.7it/s 7.9s\n",
            "                   all       1095       1341      0.703      0.588      0.632      0.343\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/15      1.46G      1.469      1.713      1.994          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.7it/s 3:34\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.9it/s 7.7s\n",
            "                   all       1095       1341      0.689      0.601      0.653      0.364\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/15      1.47G      1.421       1.54      1.935          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.9it/s 3:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.8it/s 7.9s\n",
            "                   all       1095       1341      0.696      0.639      0.663       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/15      1.49G      1.392      1.426      1.907          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 7.0it/s 3:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.2it/s 8.4s\n",
            "                   all       1095       1341       0.74       0.65      0.697      0.396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/15      1.51G      1.364      1.309      1.872          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.9it/s 3:29\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.0it/s 8.6s\n",
            "                   all       1095       1341      0.713      0.657      0.693      0.401\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/15      1.52G       1.32      1.222      1.827         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.8it/s 3:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.0it/s 8.7s\n",
            "                   all       1095       1341      0.756       0.66      0.705      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/15      1.54G      1.299      1.149      1.815          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.8it/s 3:32\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.0it/s 8.7s\n",
            "                   all       1095       1341      0.784       0.67      0.727      0.429\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/15      1.56G      1.258      1.078      1.771          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.7it/s 3:34\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.0it/s 8.7s\n",
            "                   all       1095       1341      0.798      0.686       0.74      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/15      1.57G      1.238      1.019      1.743         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.8it/s 3:32\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.1it/s 8.5s\n",
            "                   all       1095       1341      0.797      0.708      0.761      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/15      1.59G      1.203     0.9519      1.716          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.8it/s 3:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.0it/s 8.6s\n",
            "                   all       1095       1341      0.782      0.707       0.75      0.443\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/15      1.61G      1.176     0.9048      1.687          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1438/1438 6.8it/s 3:30\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 8.2it/s 8.4s\n",
            "                   all       1095       1341      0.816      0.696      0.748      0.445\n",
            "\n",
            "15 epochs completed in 0.940 hours.\n",
            "Optimizer stripped from /content/yolo_runs/yolo_effnet/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/yolo_runs/yolo_effnet/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/yolo_runs/yolo_effnet/weights/best.pt...\n",
            "Ultralytics 8.3.228 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,988 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 69/69 7.0it/s 9.8s\n",
            "                   all       1095       1341      0.797      0.707      0.762      0.446\n",
            "Speed: 0.3ms preprocess, 2.3ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/yolo_runs/yolo_effnet\u001b[0m\n",
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.1M/47.1M [00:00<00:00, 130MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n",
            "1 0.6578 0.8289 0.314 0.9105\n",
            "2 0.1471 0.9572 0.308 0.9172\n",
            "3 0.0666 0.9805 0.3488 0.9157\n",
            "4 0.0487 0.9846 0.3562 0.918\n",
            "5 0.0303 0.9912 0.4409 0.918\n",
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        }
      ],
      "source": [
        "%pip install -q ultralytics kagglehub pyyaml opencv-python efficientnet_pytorch torch torchvision\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "import yaml\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import kagglehub\n",
        "from ultralytics import YOLO\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "#**********************************************\n",
        "base_p = pathlib.Path(\"/content\") if os.path.isdir(\"/content\") else pathlib.Path.cwd()\n",
        "path = kagglehub.dataset_download(\"rupankarmajumdar/crop-pests-dataset\")\n",
        "\n",
        "local_path = pathlib.Path(\"/content/datasets/crop-pests\")\n",
        "local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copytree(path, local_path, dirs_exist_ok=True)\n",
        "\n",
        "data_yaml_path = local_path / \"data.yaml\"\n",
        "data_cfg = {\n",
        "    \"path\": str(local_path),\n",
        "    \"train\": \"train/images\",\n",
        "    \"val\":   \"valid/images\",\n",
        "    \"test\":  \"test/images\",\n",
        "    \"names\":\n",
        "     [\n",
        "      \"Ants\",\n",
        "      \"Bees\",\n",
        "      \"Beetles\",\n",
        "      \"Catterpillars\",\n",
        "      \"Earthworms\",\n",
        "      \"Earwigs\",\n",
        "      \"Grasshoppers\",\n",
        "      \"Moths\",\n",
        "      \"Slugs\",\n",
        "      \"Snails\",\n",
        "      \"Wasps\",\n",
        "      \"Weevils\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(data_yaml_path, \"w\") as f:\n",
        "    yaml.safe_dump(data_cfg, f)\n",
        "#**********************************************\n",
        "# YOLO model training\n",
        "yolo = YOLO(\"yolov8n.pt\")\n",
        "yolo.train(\n",
        "    data=str(data_yaml_path),\n",
        "    epochs=15,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    device=0,\n",
        "    workers=2,\n",
        "    project=str(base_p / \"yolo_runs\"),\n",
        "    name=\"yolo_effnet\",\n",
        "    verbose=False,\n",
        ")\n",
        "#**********************************************\n",
        "# Crop images\n",
        "def extract_crop_from_box(img, cid, xc, yc, bw, bh):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    cx = xc * w\n",
        "    cy = yc * h\n",
        "    bw_px = bw * w\n",
        "    bh_px = bh * h\n",
        "\n",
        "    x1 = int(max(0, cx - bw_px / 2))\n",
        "    y1 = int(max(0, cy - bh_px / 2))\n",
        "    x2 = int(min(w, cx + bw_px / 2))\n",
        "    y2 = int(min(h, cy + bh_px / 2))\n",
        "\n",
        "    crop = img[y1:y2, x1:x2]\n",
        "    if crop.size == 0:\n",
        "        return None\n",
        "\n",
        "    crop = cv2.resize(crop, (300, 300))\n",
        "\n",
        "    cname = data_cfg[\"names\"][cid]\n",
        "    return crop, cname\n",
        "\n",
        "#**********************************************\n",
        "\n",
        "def make_crops(split_rel, split_name):\n",
        "    img_dir = local_path / split_rel\n",
        "    lab_dir = img_dir.parent / \"labels\"\n",
        "\n",
        "    out_root = local_path / \"cls_ds\" / split_name\n",
        "    out_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    img_paths = (\n",
        "        list(img_dir.glob(\"*.jpg\"))\n",
        "        + list(img_dir.glob(\"*.png\"))\n",
        "        + list(img_dir.glob(\"*.jpeg\"))\n",
        "    )\n",
        "\n",
        "    k = 0\n",
        "    for p in img_paths:\n",
        "        lp = lab_dir / (p.stem + \".txt\")\n",
        "        if not lp.exists():\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(str(p))\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        with open(lp, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if not parts:\n",
        "                    continue\n",
        "\n",
        "                cid = int(parts[0])\n",
        "                if cid < 0 or cid >= len(data_cfg[\"names\"]):\n",
        "                    continue\n",
        "\n",
        "                xc, yc, bw, bh = map(float, parts[1:])\n",
        "\n",
        "                out = extract_crop_from_box(img, cid, xc, yc, bw, bh)\n",
        "                if out is None:\n",
        "                    continue\n",
        "\n",
        "                crop, cname = out\n",
        "\n",
        "                cls_dir = out_root / cname\n",
        "                cls_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                op = cls_dir / f\"{p.stem}_{k}.jpg\"\n",
        "                cv2.imwrite(str(op), crop)\n",
        "                k += 1\n",
        "\n",
        "tr_img = \"train/images\"\n",
        "va_img = \"valid/images\"\n",
        "\n",
        "make_crops(tr_img, \"train\")\n",
        "make_crops(va_img, \"val\")\n",
        "\n",
        "cls_root = local_path / \"cls_ds\"\n",
        "\n",
        "#**********************************************\n",
        "# load data\n",
        "tr_tf = T.Compose([\n",
        "    T.Resize((300, 300)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "va_tf = T.Compose([\n",
        "    T.Resize((300, 300)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "tr_ds = ImageFolder(str(cls_root / \"train\"), transform=tr_tf)\n",
        "va_ds = ImageFolder(str(cls_root / \"val\"), transform=va_tf)\n",
        "\n",
        "tr_loader = DataLoader(\n",
        "    tr_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "va_loader = DataLoader(\n",
        "    va_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "n_cls = len(tr_ds.classes)\n",
        "#**********************************************\n",
        "#**********************************************\n",
        "# EfficientNet-B3 Classifier\n",
        "net = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
        "fc_in = net._fc.in_features\n",
        "net._fc = nn.Linear(fc_in, n_cls)\n",
        "net = net.cuda()\n",
        "\n",
        "crit = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(net.parameters(), lr=1e-4)\n",
        "\n",
        "def train_one_epoch(m, tr_l):\n",
        "    m.train()\n",
        "    s_loss = 0.0\n",
        "    s_corr = 0\n",
        "    s_tot = 0\n",
        "\n",
        "    for x, y in tr_l:\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = m(x)\n",
        "        loss = crit(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        s_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        s_corr += pred.eq(y).sum().item()\n",
        "        s_tot += y.size(0)\n",
        "\n",
        "    tr_loss = s_loss / s_tot\n",
        "    tr_acc = s_corr / s_tot\n",
        "    return tr_loss, tr_acc\n",
        "\n",
        "#**********************************************\n",
        "def eval_one_epoch(m, va_l):\n",
        "    m.eval()\n",
        "    v_loss_sum = 0.0\n",
        "    v_corr = 0\n",
        "    v_tot = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in va_l:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            out = m(x)\n",
        "            loss = crit(out, y)\n",
        "            v_loss_sum += loss.item() * x.size(0)\n",
        "            _, pred = out.max(1)\n",
        "            v_corr += pred.eq(y).sum().item()\n",
        "            v_tot += y.size(0)\n",
        "\n",
        "    v_loss = v_loss_sum / v_tot\n",
        "    v_acc = v_corr / v_tot\n",
        "    return v_loss, v_acc\n",
        "#**********************************************\n",
        "\n",
        "def train_net(m, tr_l, va_l, epochs=5):\n",
        "    best_acc = 0.0\n",
        "    best_w = None\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(m, tr_l)\n",
        "        v_loss, v_acc = eval_one_epoch(m, va_l)\n",
        "\n",
        "        print(ep, round(tr_loss, 4), round(tr_acc, 4), round(v_loss, 4), round(v_acc, 4))\n",
        "\n",
        "        if v_acc > best_acc:\n",
        "            best_acc = v_acc\n",
        "            best_w = m.state_dict()\n",
        "\n",
        "    if best_w is not None:\n",
        "        m.load_state_dict(best_w)\n",
        "    return m\n",
        "\n",
        "\n",
        "\n",
        "net = train_net(net, tr_loader, va_loader, epochs=5)\n",
        "\n",
        "net_p = base_p / \"eff_b3_cp.pt\"\n",
        "torch.save(net.state_dict(), net_p)\n",
        "#**********************************************\n",
        "# YOLO + EffNet\n",
        "yolo.eval()\n",
        "\n",
        "net = EfficientNet.from_pretrained(\"efficientnet-b3\")\n",
        "fc_in = net._fc.in_features\n",
        "net._fc = nn.Linear(fc_in, n_cls)\n",
        "net.load_state_dict(torch.load(net_p, map_location=\"cuda\"))\n",
        "net = net.cuda()\n",
        "net.eval()\n",
        "\n",
        "inf_tf = T.Compose([\n",
        "    T.Resize((300, 300)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "idx2cls = {i: c for i, c in enumerate(tr_ds.classes)}\n",
        "\n",
        "\n",
        "def pred_one(pth, th=0.25):\n",
        "    pth = str(pth)\n",
        "    img_bgr = cv2.imread(pth)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(\"bad img \" + str(pth))\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    r = yolo(img_rgb, imgsz=640)[0]\n",
        "    out = []\n",
        "\n",
        "    for b in r.boxes:\n",
        "        d_conf = float(b.conf.cpu().item())\n",
        "        if d_conf < th:\n",
        "            continue\n",
        "\n",
        "        x1, y1, x2, y2 = b.xyxy[0].cpu().numpy().astype(int)\n",
        "        h, w = img_bgr.shape[:2]\n",
        "        x1 = max(0, min(x1, w - 1))\n",
        "        x2 = max(0, min(x2, w - 1))\n",
        "        y1 = max(0, min(y1, h - 1))\n",
        "        y2 = max(0, min(y2, h - 1))\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        crop_bgr = img_bgr[y1:y2, x1:x2]\n",
        "        if crop_bgr.size == 0:\n",
        "            continue\n",
        "\n",
        "        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
        "        crop_pil = T.functional.to_pil_image(crop_rgb)\n",
        "        inp = inf_tf(crop_pil).unsqueeze(0).cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = net(inp)\n",
        "            probs = torch.softmax(logits, dim=1)[0]\n",
        "            cid = int(torch.argmax(probs).item())\n",
        "            c_conf = float(probs[cid].item())\n",
        "            cname = idx2cls[cid]\n",
        "\n",
        "        out.append({\n",
        "            \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
        "            \"cls\": cname,\n",
        "            \"det_conf\": d_conf,\n",
        "            \"cls_conf\": c_conf,\n",
        "        })\n",
        "\n",
        "    return out\n",
        "#**********************************************"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPZIEAl6Ulld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}